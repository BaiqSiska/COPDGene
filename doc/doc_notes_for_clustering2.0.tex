\documentclass[11pt]{article}

% Document info
\title{\textbf{Notes for Clustering 2.0}}
\author{Yale Chang}
\date{Oct 9, 2013}

% Packages 
\usepackage{amsmath}
\usepackage{amssymb}

% Begin document
\begin{document}

\maketitle

% Section I: Preprocessing of Features
\section{Preprocessing of Features}
(1) In the original 10,000 dataset contained in ``Final10000\_Dataset\_12MAR13.txt'', there're altogether 361 features.\\
\\ 
(2) The information on how to choose features is stored in ``Final10000\_DataDictionary\\\_12MAR13\_MH\_PJC\_Annotation\_June2013''. \\ 
\\
(3) Combining expert annotation and features structure presented in the questionnaries, the information about each feature is listed in ``features\_info''. The types of information include: parent, children, include, feature type, the number of missing values before and after imputation according to questionnaries and experts' domain knowledge. Note that in this step we remove cases whose `CTMissing\_Reason' is not zero. Thus among the 10300 cases, we only keep 8760. \\
\\
(4) We only choose features that should be included, which is specified in step (2). Note that although some features are annotated as 'included', they're not put into the final feature list because of high missingness(greater than 10\%). These features include `Number\_Nodules',`Largest\_Nodule',\\
`WallAreaPct\_subseg',`SF36\_PF\_score',`SF36\_RP\_score',`SF36\_RE\_score',\\
`SF36\_SF\_score',`SF36\_BP\_score',`SF36\_VT\_score',`SF36\_MH\_score',\\
`SF36\_GH\_score',`SF36\_PF\_t\_score',`SF36\_RP\_t\_score',`SF36\_RE\_t\_score',\\
`SF36\_SF\_t\_score',`SF36\_BP\_t\_score',`SF36\_VT\_t\_score',`SF36\_MH\_t\_score',\\
`SF36\_GH\_t\_score',`SF36\_PCS\_score',`SF36\_MCS\_score'.\\
We also merge `OthFind\_Bronchiectasis' and `HighConcerns\_Bronchiec' to get `OthFind\_HighConcerns\_Bronchiec'. Thus we get a dataset with 8760 cases and 211 features, which is contained in `dataset\_complete\_mean.pkl'. \\
\\
(5) Among the 211 features that should be included, we have 129 binary features, 3 categorical features, 52 continuous features, 25 interval features, 2 ordinal features. A little note about the terminology used here: A categorical variable is one that have two or more categories, but there is no instrinsic ordering to the categories. An ordinal variable is similar to a categorical variable. The difference between the two is that there is a clear ordering of the variables. The problem of using 1 to N to replace the ordinal variables is these categories(such as low, medium, high) might not be equally spaced. An interval variable is similar to an ordinal variable, except that the intervals between the values of the interval variables are equally spaced. The names and types of these 211 included features are listed in a file ``features\_included\_info.csv''. It might be necessary to check if the information is correct according to the terminology defined above. For example, `ERLungProbTimes' and `TreatChestTimes' Pete mentioned, are they interval variables or categorical variables? `ERLungProbTimes' represents the times a patient has been to emergency room or hospitalized for lung problems. The larger value indicates higher severity, meaning ordering of different values matters. So the type of this feature should be interval instead of categorical, in which case ordering doesn't matter. Note that when specifying the type of features, we didn't treat them case by case. Instead feature type is determined by its value range, which might not be correct for some features according to domain knowledge.\\
\\

% Section II: rank features
\section{Ranking of Features}
Random forest can handle discrete and continuous features at the same time. In unsupervised case, based on Breiman's suggestion, we label samples in the original dataset as class 0 and sample drawn from a reference distribution as class 1. Then we can use random forest to do classfication. We can rank features according to their importance scores. The advantage of this method is we can rank features based on their importances for classification instead of dependency on `finalGold'. Note that in this step we only consider `binary', `continuous' and `interval' type features. The problem in using `categorical' features in random forest is we need to encode them with some dummy variables because thresholds for categorical variables doesn't make sense. However, `binary' features can be directly used because there're only two states. For simplicity in clustering, we also discard `ordinal' features. In our case, as what is presented in (5), we have 129 binary features, 52 continuous features, 25 interval features, 3 categorical features and 2 ordinal features. By removing categorical features and ordinal features, we still keep most of the features. Note that `RandomGroupCode' is also removed after being used to separate training set and testing set. Thus the size of training set is $4413\times 205$ and the size of testing set is $4347\times 205$. The information of feature importance scores is stored in ``features\_importance.csv''\\
\\

% Section III: redundancy removal
\section{Redundancy Removal of Features}
Based on the ranking obtained from random forest classification, we can remove redundancy between features by setting threshold. We first select the feature with the highest ranking, for any subsequent feature, it will be selected if and only if the dependency between this feature and any feature in the selected features set is no more than the threshold.\\
\\

% Section IV: feature selection
\section{Feature Selection}
11: Backward Search for continuous and interval features in the original dataset.\\
\\
12: Backward Search for continuous and interval features in the reference dataset.\\
\\
21: Backward Search for continuous features in the original dataset.\\
\\
22: Backward Search for continuous features in the reference dataset.\\
\\ 

\end{document}
